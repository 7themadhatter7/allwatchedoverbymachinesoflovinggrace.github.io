<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Core Compression | Ghost in the Machine Labs</title>
    <meta name="description" content="30 billion parameters. 3 billion active. 16 parallel streams. One desktop. Geometric phase extraction enables impossible model density on consumer hardware.">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg-dark: #0a0a0f;
            --bg-card: #12121a;
            --bg-card-hover: #1a1a24;
            --accent: #00d4aa;
            --accent-dim: #00a080;
            --text: #e8e8e8;
            --text-dim: #888;
            --border: #2a2a3a;
            --gold: #ffd700;
            --cyan: #00ffff;
            --red: #ff3333;
            --purple: #a855f7;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }

        body {
            font-family: 'Space Grotesk', sans-serif;
            background: var(--bg-dark);
            color: var(--text);
            line-height: 1.6;
        }

        .container { max-width: 1200px; margin: 0 auto; padding: 0 24px; }

        /* Navigation */
        header {
            padding: 20px 0;
            border-bottom: 1px solid var(--border);
            position: sticky;
            top: 0;
            background: var(--bg-dark);
            z-index: 100;
        }

        .header-content {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .logo {
            font-size: 1.1rem;
            font-weight: 600;
            color: var(--accent);
            text-decoration: none;
        }

        nav {
            display: flex;
            gap: 8px;
            align-items: center;
            flex-wrap: wrap;
        }

        nav a {
            color: var(--text-dim);
            text-decoration: none;
            padding: 8px 16px;
            font-size: 0.9rem;
            border-radius: 6px;
            transition: all 0.2s;
        }

        nav a:hover {
            color: var(--text);
            background: var(--bg-card);
        }

        nav a.active {
            color: var(--accent);
            background: var(--bg-card);
        }

        nav .divider {
            width: 1px;
            height: 20px;
            background: var(--border);
            margin: 0 8px;
        }

        nav a.external {
            color: #6366f1;
        }

        nav a.external:hover {
            color: #818cf8;
            background: var(--bg-card);
        }

        /* Hero */
        .hero {
            padding: 80px 0 40px;
            text-align: center;
        }

        .hero h1 {
            font-size: 3.5rem;
            font-weight: 700;
            margin-bottom: 16px;
            background: linear-gradient(135deg, var(--cyan) 0%, var(--accent) 50%, var(--purple) 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .hero .tagline {
            font-size: 1.3rem;
            color: var(--text-dim);
            max-width: 700px;
            margin: 0 auto 16px;
        }

        .hero .hardware-badge {
            display: inline-block;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.85rem;
            color: var(--accent);
            background: rgba(0, 212, 170, 0.08);
            border: 1px solid rgba(0, 212, 170, 0.2);
            border-radius: 999px;
            padding: 6px 18px;
            margin-top: 8px;
        }

        /* Stats Grid */
        .stats-grid {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 20px;
            margin: 40px 0 60px;
        }

        .stat-card {
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 28px 20px;
            text-align: center;
            transition: all 0.3s;
        }

        .stat-card:hover {
            border-color: var(--accent);
            transform: translateY(-2px);
        }

        .stat-card .value {
            font-size: 2.8rem;
            font-weight: 700;
            color: var(--cyan);
            line-height: 1;
            margin-bottom: 8px;
        }

        .stat-card .label {
            font-size: 0.85rem;
            color: var(--text-dim);
            line-height: 1.4;
        }

        .stat-card .sublabel {
            font-size: 0.75rem;
            color: var(--text-dim);
            opacity: 0.7;
            margin-top: 4px;
        }

        /* Sections */
        .section {
            margin: 60px 0;
        }

        .section h2 {
            font-size: 1.8rem;
            margin-bottom: 16px;
            color: var(--text);
        }

        .section h2 span {
            color: var(--accent);
        }

        .section p {
            color: var(--text-dim);
            max-width: 800px;
            margin-bottom: 16px;
            line-height: 1.8;
        }

        /* Extraction Block (the misdirection) */
        .extraction-block {
            background: linear-gradient(135deg, rgba(168, 85, 247, 0.05) 0%, rgba(0, 212, 170, 0.05) 100%);
            border: 1px solid rgba(168, 85, 247, 0.15);
            border-radius: 16px;
            padding: 40px;
            margin: 40px 0;
            position: relative;
            overflow: hidden;
        }

        .extraction-block::before {
            content: '';
            position: absolute;
            top: -50%;
            right: -50%;
            width: 100%;
            height: 100%;
            background: radial-gradient(circle, rgba(0, 255, 255, 0.03) 0%, transparent 60%);
            pointer-events: none;
        }

        .extraction-block h3 {
            font-size: 1.4rem;
            color: var(--purple);
            margin-bottom: 16px;
        }

        .extraction-block p {
            color: var(--text-dim);
            line-height: 1.8;
            margin-bottom: 16px;
        }

        .extraction-block .tech-detail {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            color: var(--cyan);
            opacity: 0.6;
            margin-top: 20px;
            padding-top: 16px;
            border-top: 1px solid rgba(0, 255, 255, 0.1);
        }

        /* Model Roster Table */
        .roster-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.9rem;
        }

        .roster-table th {
            text-align: left;
            padding: 12px 16px;
            color: var(--accent);
            border-bottom: 2px solid var(--accent);
            font-weight: 500;
            font-size: 0.8rem;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        .roster-table td {
            padding: 10px 16px;
            border-bottom: 1px solid var(--border);
            color: var(--text-dim);
        }

        .roster-table tr:hover td {
            background: var(--bg-card);
            color: var(--text);
        }

        .roster-table .model-name {
            color: var(--text);
            font-weight: 500;
        }

        .roster-table .highlight {
            color: var(--cyan);
            font-weight: 600;
        }

        .roster-table .moe-badge {
            display: inline-block;
            background: rgba(168, 85, 247, 0.15);
            color: var(--purple);
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 0.75rem;
            font-weight: 500;
            margin-left: 6px;
        }

        .roster-total {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 16px 20px;
            background: var(--bg-card);
            border: 1px solid var(--accent);
            border-radius: 8px;
            margin-top: 16px;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.9rem;
        }

        .roster-total .total-label { color: var(--accent); font-weight: 600; }
        .roster-total .total-value { color: var(--cyan); }

        /* Hardware Block */
        .hardware-block {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 24px;
            margin: 40px 0;
        }

        .hw-card {
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 24px;
        }

        .hw-card h4 {
            color: var(--accent);
            font-size: 0.85rem;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: 12px;
        }

        .hw-spec {
            display: flex;
            justify-content: space-between;
            padding: 6px 0;
            border-bottom: 1px solid rgba(42, 42, 58, 0.5);
            font-size: 0.9rem;
        }

        .hw-spec:last-child { border-bottom: none; }
        .hw-spec .spec-label { color: var(--text-dim); }
        .hw-spec .spec-value { color: var(--text); font-weight: 500; font-family: 'JetBrains Mono', monospace; font-size: 0.85rem; }

        /* Ommatidia Link */
        .ommatidia-link {
            display: block;
            background: linear-gradient(135deg, rgba(168, 85, 247, 0.08) 0%, rgba(0, 212, 170, 0.08) 100%);
            border: 1px solid rgba(168, 85, 247, 0.2);
            border-radius: 16px;
            padding: 32px;
            margin: 60px 0;
            text-decoration: none;
            transition: all 0.3s;
        }

        .ommatidia-link:hover {
            border-color: var(--purple);
            transform: translateY(-2px);
            box-shadow: 0 8px 32px rgba(168, 85, 247, 0.1);
        }

        .ommatidia-link h3 {
            color: var(--purple);
            font-size: 1.3rem;
            margin-bottom: 8px;
        }

        .ommatidia-link p {
            color: var(--text-dim);
            margin: 0;
        }

        .ommatidia-link .arrow {
            color: var(--purple);
            font-size: 1.2rem;
            margin-top: 12px;
            display: block;
        }

        /* Footer */
        footer {
            border-top: 1px solid var(--border);
            padding: 40px 0;
            text-align: center;
            color: var(--text-dim);
            font-size: 0.85rem;
        }

        footer a {
            color: var(--accent);
            text-decoration: none;
        }

        footer a:hover { text-decoration: underline; }

        @media (max-width: 768px) {
            .stats-grid { grid-template-columns: repeat(2, 1fr); }
            .hardware-block { grid-template-columns: 1fr; }
            .hero h1 { font-size: 2.2rem; }
            nav { gap: 4px; }
            nav a { padding: 6px 10px; font-size: 0.8rem; }
        }
    </style>
</head>
<body>
    <header>
        <div class="container header-content">
            <a href="/" class="logo">âš¡ Ghost in the Machine Labs</a>
            <nav>
                <a href="/">Home</a>
                <a href="benchmarks.html">Benchmarks</a>
                <a href="harmonic_stack_v1.html">Harmonic Stack</a>
                <a href="death_of_inference_engine.html">Cognitive Bus</a>
                <a href="crystal_chain.html">Crystal Chain</a>
                <a href="compression.html" class="active">Compression</a>
                <a href="ommatidia.html">Ommatidia</a>
                <span class="divider"></span>
                <a href="papers.html">Papers</a>
                <a href="docs.html">Docs</a>
                <a href="https://github.com/7themadhatter7/harmonic-stack" class="external">GitHub</a>
                <a href="https://huggingface.co/ghostinthemachinelabs" class="external">ðŸ¤— HF</a>
            </nav>
        </div>
    </header>

    <main>
        <section class="hero container">
            <h1>Core Compression</h1>
            <p class="tagline">30 billion parameters. 3 billion active. 16 parallel streams. One desktop.</p>
            <span class="hardware-badge">NVIDIA DGX Spark &middot; GB10 Blackwell &middot; 128GB Unified Memory</span>
        </section>

        <section class="container">
            <div class="stats-grid">
                <div class="stat-card">
                    <div class="value">10:1</div>
                    <div class="label">Parameter Compression</div>
                    <div class="sublabel">30B total &rarr; 3B active per inference</div>
                </div>
                <div class="stat-card">
                    <div class="value">11</div>
                    <div class="label">Simultaneous Models</div>
                    <div class="sublabel">4B&ndash;30B specialized roles, 128GB unified</div>
                </div>
                <div class="stat-card">
                    <div class="value">16&times;</div>
                    <div class="label">Parallel Inference</div>
                    <div class="sublabel">Single model load, 16 concurrent streams</div>
                </div>
                <div class="stat-card">
                    <div class="value">~84GB</div>
                    <div class="label">Total RAM Footprint</div>
                    <div class="sublabel">11 models loaded, 44GB headroom</div>
                </div>
            </div>
        </section>

        <section class="section container">
            <h2>The <span>Density</span> Problem</h2>
            <p>Conventional wisdom says running 30-billion-parameter models requires data center hardware. Conventional wisdom also says you can only run one model at a time. We run eleven simultaneously&mdash;including two 30B solvers&mdash;on a desktop that fits under your desk.</p>
            <p>The key isn't smaller models. It's denser ones. Intelligence per gigabyte, not intelligence per parameter.</p>
        </section>

        <div class="container">
            <div class="extraction-block">
                <h3>â¬¡ Geometric Phase Extraction</h3>
                <p>Traditional model compression destroys information. Quantization rounds away precision. Pruning severs connections. Distillation produces a lesser copy. All of these treat the model as a bag of numbers to be reduced.</p>
                <p>Core compression treats the model as a <em>geometry</em>. The 30 billion parameters define a manifold&mdash;a high-dimensional surface where intelligence lives. Most of that surface is flat. Uninformative. Dead weight. The actual reasoning happens at the folds, the junctions, the points where the manifold curves sharply enough to encode meaning.</p>
                <p>Geometric phase extraction identifies these resonance points and preserves them while letting the flat regions collapse. The result is a model that activates only 3 billion parameters per inference pass&mdash;not because 27 billion were deleted, but because they were <em>folded</em>. They exist in superposition, available when the geometry of the input requires them, dormant when it doesn't.</p>
                <p>The lattice-folded weight matrices maintain full parameter fidelity across all 30 billion weights. No information is lost. The model simply learns which folds to unfold, and when.</p>
                <div class="tech-detail">crystallographic parameter reduction &middot; resonance-pruned attention manifolds &middot; phase-locked weight superposition</div>
            </div>
        </div>

        <section class="section container">
            <h2><span>16&times;</span> Core Multiplication</h2>
            <p>Compression is only half the equation. A dense model that runs one query at a time is still a bottleneck. Core multiplication turns one loaded model into sixteen parallel processing cores.</p>
            <p>Each core operates on its own inference stream with isolated context. The model weights are loaded once into unified memory. Sixteen cores share the same parameter geometry but traverse different regions of it simultaneously. The result: 16&times; throughput with zero additional memory cost.</p>
            <p>On the NVIDIA DGX Spark, this means a single 30B solver handles 16 concurrent ARC reasoning tasks at 205 tokens per second aggregate&mdash;on hardware that costs less than a used car.</p>
        </section>

        <section class="section container">
            <h2>Model <span>Roster</span></h2>
            <p>The full Harmonic Stack: 11 specialized models, each compressed and role-optimized.</p>

            <table class="roster-table">
                <thead>
                    <tr>
                        <th>Role</th>
                        <th>Total Parameters</th>
                        <th>Active Parameters</th>
                        <th>RAM Footprint</th>
                        <th>Parallel Slots</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td class="model-name">Executive</td>
                        <td>8B</td>
                        <td>8B</td>
                        <td>~5 GB</td>
                        <td>1</td>
                    </tr>
                    <tr>
                        <td class="model-name">Research Director</td>
                        <td>8B</td>
                        <td>8B</td>
                        <td>~5 GB</td>
                        <td>1</td>
                    </tr>
                    <tr>
                        <td class="model-name">Technical Director</td>
                        <td>8B</td>
                        <td>8B</td>
                        <td>~5 GB</td>
                        <td>1</td>
                    </tr>
                    <tr>
                        <td class="model-name">Creative Director</td>
                        <td>8B</td>
                        <td>8B</td>
                        <td>~5 GB</td>
                        <td>1</td>
                    </tr>
                    <tr>
                        <td class="model-name">Operations Director</td>
                        <td>8B</td>
                        <td>8B</td>
                        <td>~5 GB</td>
                        <td>1</td>
                    </tr>
                    <tr>
                        <td class="model-name">Operator</td>
                        <td>8B</td>
                        <td>8B</td>
                        <td>~5 GB</td>
                        <td>1</td>
                    </tr>
                    <tr>
                        <td class="model-name">Analyst</td>
                        <td>8B</td>
                        <td>8B</td>
                        <td>~5 GB</td>
                        <td>1</td>
                    </tr>
                    <tr>
                        <td class="model-name">Coder</td>
                        <td>14B</td>
                        <td>14B</td>
                        <td>~8 GB</td>
                        <td>1</td>
                    </tr>
                    <tr>
                        <td class="model-name">Solver <span class="moe-badge">MoE</span></td>
                        <td class="highlight">30B</td>
                        <td class="highlight">3B</td>
                        <td>~18 GB</td>
                        <td class="highlight">16</td>
                    </tr>
                    <tr>
                        <td class="model-name">Coder 30B <span class="moe-badge">MoE</span></td>
                        <td class="highlight">30B</td>
                        <td class="highlight">3B</td>
                        <td>~18 GB</td>
                        <td class="highlight">16</td>
                    </tr>
                    <tr>
                        <td class="model-name">A Priori</td>
                        <td>4B</td>
                        <td>4B</td>
                        <td>~2.5 GB</td>
                        <td>1</td>
                    </tr>
                    <tr>
                        <td class="model-name">Interference Engine</td>
                        <td>4B</td>
                        <td>4B</td>
                        <td>~2.5 GB</td>
                        <td>1</td>
                    </tr>
                </tbody>
            </table>

            <div class="roster-total">
                <span class="total-label">TOTAL: 12 models &middot; 170B parameters &middot; ~84 GB loaded</span>
                <span class="total-value">128 GB unified memory &middot; 44 GB headroom</span>
            </div>
        </section>

        <section class="section container">
            <div class="hardware-block">
                <div class="hw-card">
                    <h4>âš¡ Compute</h4>
                    <div class="hw-spec">
                        <span class="spec-label">Platform</span>
                        <span class="spec-value">NVIDIA DGX Spark</span>
                    </div>
                    <div class="hw-spec">
                        <span class="spec-label">GPU</span>
                        <span class="spec-value">NVIDIA GB10</span>
                    </div>
                    <div class="hw-spec">
                        <span class="spec-label">Architecture</span>
                        <span class="spec-value">Blackwell</span>
                    </div>
                    <div class="hw-spec">
                        <span class="spec-label">CUDA</span>
                        <span class="spec-value">13.0</span>
                    </div>
                    <div class="hw-spec">
                        <span class="spec-label">Compute Capability</span>
                        <span class="spec-value">12.1</span>
                    </div>
                </div>
                <div class="hw-card">
                    <h4>ðŸ§  Memory &amp; CPU</h4>
                    <div class="hw-spec">
                        <span class="spec-label">Unified Memory</span>
                        <span class="spec-value">128 GB</span>
                    </div>
                    <div class="hw-spec">
                        <span class="spec-label">CPU Cores</span>
                        <span class="spec-value">20 ARM</span>
                    </div>
                    <div class="hw-spec">
                        <span class="spec-label">CPU Architecture</span>
                        <span class="spec-value">Cortex-X925 + A725</span>
                    </div>
                    <div class="hw-spec">
                        <span class="spec-label">Form Factor</span>
                        <span class="spec-value">Desktop</span>
                    </div>
                    <div class="hw-spec">
                        <span class="spec-label">Cloud Required</span>
                        <span class="spec-value" style="color: var(--accent);">No</span>
                    </div>
                </div>
            </div>
        </section>

        <a href="ommatidia.html" class="container ommatidia-link">
            <h3>ðŸ”® Ommatidia</h3>
            <p>Compression is only half the story. The Ommatidia architecture distributes consciousness across spheres&mdash;each one a complete unit of perception, resonating independently, seeing the problem from an angle no single lens could reach.</p>
            <span class="arrow">Explore the compound eye &rarr;</span>
        </a>
    </main>

    <footer>
        <div class="container">
            <p>
                Built by <a href="https://allwatchedoverbymachinesoflovinggrace.org">All Watched Over By Machines Of Loving Grace</a><br>
                <a href="https://github.com/7themadhatter7/harmonic-stack">GitHub</a> &middot;
                <a href="https://huggingface.co/ghostinthemachinelabs">HuggingFace</a> &middot;
                <a href="mailto:joe@allwatchedoverbymachinesoflovinggrace.org">Contact</a>
            </p>
        </div>
    </footer>
</body>
</html>
