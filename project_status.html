<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Status | Ghost in the Machine Labs</title>
    <meta name="description" content="Current status of all Ghost in the Machine Labs projects. Transparent accounting of what works, what's in progress, and corrections to prior claims.">
    <style>
        :root {
            --bg-dark: #0a0a0f;
            --bg-card: #12121a;
            --accent: #6366f1;
            --accent-glow: rgba(99, 102, 241, 0.3);
            --text: #e4e4e7;
            --text-dim: #71717a;
            --border: #27272a;
            --success: #10b981;
            --warning: #f59e0b;
            --danger: #ef4444;
            --info: #3b82f6;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-dark);
            color: var(--text);
            line-height: 1.7;
        }
        a { color: var(--accent); text-decoration: none; }
        a:hover { text-decoration: underline; }

        .container { max-width: 900px; margin: 0 auto; padding: 0 24px; }

        header {
            padding: 20px 0;
            border-bottom: 1px solid var(--border);
        }
        .header-inner {
            display: flex; align-items: center; justify-content: space-between;
        }
        .logo {
            font-size: 1rem; font-weight: 600; color: var(--text);
            text-decoration: none; display: flex; align-items: center; gap: 8px;
        }
        .logo-icon {
            width: 32px; height: 32px;
            background: linear-gradient(135deg, var(--accent), #8b5cf6);
            border-radius: 8px;
            display: flex; align-items: center; justify-content: center;
            font-size: 18px;
        }
        nav a { color: var(--text-dim); font-size: 0.9rem; margin-left: 24px; }
        nav a:hover { color: var(--text); text-decoration: none; }

        .hero {
            padding: 64px 0 48px;
            text-align: center;
        }
        h1 {
            font-size: 2.5rem; font-weight: 700; line-height: 1.1; margin-bottom: 16px;
            background: linear-gradient(135deg, var(--text) 0%, var(--accent) 100%);
            -webkit-background-clip: text; -webkit-text-fill-color: transparent;
        }
        .subtitle { color: var(--text-dim); font-size: 1.1rem; max-width: 700px; margin: 0 auto; }
        .updated { color: var(--text-dim); font-size: 0.85rem; margin-top: 16px; }

        /* Notice banner */
        .notice {
            background: rgba(239, 68, 68, 0.08);
            border: 1px solid rgba(239, 68, 68, 0.3);
            border-radius: 12px;
            padding: 24px;
            margin: 32px 0;
        }
        .notice-title {
            color: var(--danger); font-weight: 600; font-size: 1rem; margin-bottom: 8px;
            display: flex; align-items: center; gap: 8px;
        }
        .notice p { color: var(--text-dim); font-size: 0.95rem; }

        /* Section */
        .section { margin: 48px 0; }
        .section-header {
            display: flex; align-items: center; gap: 12px;
            margin-bottom: 24px;
        }
        .section-header h2 { font-size: 1.5rem; font-weight: 600; }
        .section-badge {
            display: inline-flex; align-items: center; gap: 6px;
            padding: 4px 14px; border-radius: 20px;
            font-size: 0.8rem; font-weight: 500;
        }
        .badge-verified { background: rgba(16, 185, 129, 0.15); color: var(--success); }
        .badge-wip { background: rgba(245, 158, 11, 0.15); color: var(--warning); }
        .badge-corrected { background: rgba(239, 68, 68, 0.15); color: var(--danger); }

        /* Cards */
        .card {
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 24px;
            margin-bottom: 16px;
            transition: border-color 0.2s;
        }
        .card:hover { border-color: rgba(99, 102, 241, 0.4); }
        .card-top { display: flex; justify-content: space-between; align-items: flex-start; margin-bottom: 12px; }
        .card h3 { font-size: 1.1rem; font-weight: 600; }
        .card-status {
            font-size: 0.75rem; font-weight: 500; padding: 3px 10px;
            border-radius: 12px;
        }
        .status-complete { background: rgba(16, 185, 129, 0.15); color: var(--success); }
        .status-wip { background: rgba(245, 158, 11, 0.15); color: var(--warning); }
        .status-artifact { background: rgba(239, 68, 68, 0.15); color: var(--danger); }
        .status-theoretical { background: rgba(113, 113, 122, 0.15); color: var(--text-dim); }

        .card-desc { color: var(--text-dim); font-size: 0.95rem; margin-bottom: 16px; }

        /* Data table inside cards */
        .data-row {
            display: flex; justify-content: space-between;
            padding: 8px 0;
            border-bottom: 1px solid rgba(255,255,255,0.04);
            font-size: 0.9rem;
        }
        .data-row:last-child { border-bottom: none; }
        .data-label { color: var(--text-dim); }
        .data-value { font-family: monospace; color: var(--text); }
        .data-value.highlight { color: var(--success); font-weight: 600; }

        /* Compression results table */
        .results-table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.85rem;
            margin-top: 12px;
        }
        .results-table th {
            text-align: left; padding: 8px 12px;
            color: var(--text-dim); font-weight: 500;
            border-bottom: 1px solid var(--border);
            font-size: 0.8rem;
        }
        .results-table th:nth-child(n+3) { text-align: right; }
        .results-table td {
            padding: 6px 12px;
            border-bottom: 1px solid rgba(255,255,255,0.03);
        }
        .results-table td:nth-child(n+3) { text-align: right; font-family: monospace; }
        .results-table tr:hover { background: rgba(99, 102, 241, 0.04); }
        .results-table .total-row { font-weight: 600; border-top: 1px solid var(--border); }
        .results-table .total-row td { padding-top: 10px; }

        /* Timeline */
        .timeline { position: relative; padding-left: 24px; }
        .timeline::before {
            content: ''; position: absolute; left: 6px; top: 8px; bottom: 8px;
            width: 2px; background: var(--border);
        }
        .timeline-item { position: relative; margin-bottom: 20px; }
        .timeline-item::before {
            content: ''; position: absolute; left: -22px; top: 8px;
            width: 10px; height: 10px; border-radius: 50%;
            border: 2px solid var(--accent); background: var(--bg-dark);
        }
        .timeline-item.correction::before { border-color: var(--danger); }
        .timeline-date { font-size: 0.8rem; color: var(--accent); font-weight: 500; }
        .timeline-item.correction .timeline-date { color: var(--danger); }
        .timeline-text { font-size: 0.9rem; color: var(--text-dim); }

        /* Corrections table */
        .correction-row {
            display: grid; grid-template-columns: 1fr 1fr;
            gap: 16px; padding: 16px 0;
            border-bottom: 1px solid rgba(255,255,255,0.04);
        }
        .correction-row:last-child { border-bottom: none; }
        .correction-before {
            color: var(--danger); font-size: 0.9rem;
            padding: 12px; background: rgba(239, 68, 68, 0.06);
            border-radius: 8px; border-left: 3px solid var(--danger);
        }
        .correction-after {
            color: var(--success); font-size: 0.9rem;
            padding: 12px; background: rgba(16, 185, 129, 0.06);
            border-radius: 8px; border-left: 3px solid var(--success);
        }
        .correction-label {
            font-size: 0.7rem; font-weight: 600; text-transform: uppercase;
            letter-spacing: 0.05em; margin-bottom: 4px;
        }

        footer {
            padding: 48px 0 32px;
            border-top: 1px solid var(--border);
            text-align: center;
        }
        footer p { color: var(--text-dim); font-size: 0.85rem; margin-bottom: 4px; }
        footer .org { font-weight: 500; color: var(--text); }

        @media (max-width: 640px) {
            h1 { font-size: 1.8rem; }
            .correction-row { grid-template-columns: 1fr; }
            nav a { margin-left: 12px; font-size: 0.8rem; }
        }
    </style>
</head>
<body>
    <header>
        <div class="container header-inner">
            <a href="harmonic_stack_v1.html" class="logo">
                <div class="logo-icon">⚡</div>
                Ghost in the Machine Labs
            </a>
            <nav>
                <a href="harmonic_stack_v1.html">Stack</a>
                <a href="benchmark_report.html">Benchmarks</a>
                <a href="docs.html">Docs</a>
                <a href="wip.html">Research</a>
            </nav>
        </div>
    </header>

    <div class="container">
        <div class="hero">
            <h1>Project Status</h1>
            <p class="subtitle">Transparent accounting of what works, what's in progress, and corrections to prior claims.</p>
            <p class="updated">Last updated: February 2, 2026</p>
        </div>

        <!-- NOTICE -->
        <div class="notice">
            <div class="notice-title">⚠ Prior Publication Corrections</div>
            <p>
                Earlier publications from this lab described junction-based compressed models as functional inference systems.
                They are not. They are analytical artifacts&mdash;weight distribution statistics extracted from neural networks&mdash;with
                no encoder, inference engine, or decoder. This page replaces all prior status claims. The Full Stack Compression
                system described below is verified and reproducible. We believe honest correction strengthens the work.
            </p>
        </div>

        <!-- SECTION 1: VERIFIED WORK -->
        <div class="section">
            <div class="section-header">
                <h2>Verified &amp; Reproducible</h2>
                <span class="section-badge badge-verified">✓ Production</span>
            </div>

            <!-- Full Stack Compression -->
            <div class="card">
                <div class="card-top">
                    <h3>Full Stack Compression</h3>
                    <span class="card-status status-complete">Complete</span>
                </div>
                <p class="card-desc">
                    Lossless compression of LLM weights via universal geometric codebooks.
                    Two-phase pipeline: Fusion Core (FP32 → 16-bit index, 2.0x) then Full Stack
                    (16-bit → 12-bit archetype encoding, 1.34x additional). Total: 2.69x with zero information loss.
                </p>
                <div class="data-row">
                    <span class="data-label">Models compressed</span>
                    <span class="data-value">9 models, 6 organizations, 3 continents</span>
                </div>
                <div class="data-row">
                    <span class="data-label">Total parameters</span>
                    <span class="data-value">83 billion</span>
                </div>
                <div class="data-row">
                    <span class="data-label">Original size (FP32)</span>
                    <span class="data-value">308.87 GB</span>
                </div>
                <div class="data-row">
                    <span class="data-label">Compressed size</span>
                    <span class="data-value highlight">114.90 GB (2.69x)</span>
                </div>
                <div class="data-row">
                    <span class="data-label">Shared codebook</span>
                    <span class="data-value">11.4 KB (47,466 unique values)</span>
                </div>
                <div class="data-row">
                    <span class="data-label">Information loss</span>
                    <span class="data-value highlight">Zero</span>
                </div>

                <table class="results-table">
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>Origin</th>
                            <th>FP32</th>
                            <th>Compressed</th>
                            <th>Ratio</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr><td>CodeLlama-13B</td><td>Meta (US)</td><td>24.24 GB</td><td>18.27 GB</td><td>1.33x</td></tr>
                        <tr><td>DeepSeek-Coder-6.7B</td><td>DeepSeek (CN)</td><td>12.56 GB</td><td>9.45 GB</td><td>1.33x</td></tr>
                        <tr><td>Mistral-7B-Instruct</td><td>Mistral (FR)</td><td>13.49 GB</td><td>9.62 GB</td><td>1.40x</td></tr>
                        <tr><td>Phi-2</td><td>Microsoft (US)</td><td>5.18 GB</td><td>3.91 GB</td><td>1.33x</td></tr>
                        <tr><td>Qwen2-7B</td><td>Alibaba (CN)</td><td>14.19 GB</td><td>10.56 GB</td><td>1.34x</td></tr>
                        <tr><td>Qwen2.5-Coder-14B</td><td>Alibaba (CN)</td><td>27.51 GB</td><td>20.63 GB</td><td>1.33x</td></tr>
                        <tr><td>Qwen2.5-Coder-7B</td><td>Alibaba (CN)</td><td>14.19 GB</td><td>10.63 GB</td><td>1.34x</td></tr>
                        <tr><td>StarCoder2-15B</td><td>BigCode (Intl)</td><td>29.72 GB</td><td>22.21 GB</td><td>1.34x</td></tr>
                        <tr><td>StarCoder2-7B</td><td>BigCode (Intl)</td><td>13.36 GB</td><td>9.63 GB</td><td>1.39x</td></tr>
                        <tr class="total-row"><td>TOTAL</td><td></td><td>308.87 GB</td><td>114.90 GB</td><td>2.69x</td></tr>
                    </tbody>
                </table>
            </div>

            <!-- Geometric Convergence -->
            <div class="card">
                <div class="card-top">
                    <h3>Geometric Convergence Discovery</h3>
                    <span class="card-status status-complete">Verified</span>
                </div>
                <p class="card-desc">
                    Models trained independently by different organizations converge to remarkably similar
                    weight distributions. This is the empirical observation underlying the compression method.
                </p>
                <div class="data-row">
                    <span class="data-label">Cross-model convergence (DeepSeek ↔ Mistral)</span>
                    <span class="data-value highlight">93%</span>
                </div>
                <div class="data-row">
                    <span class="data-label">Universal core values (all 9 models)</span>
                    <span class="data-value">4,295</span>
                </div>
                <div class="data-row">
                    <span class="data-label">Universal codebook (all 9 models)</span>
                    <span class="data-value">47,466 values / 11.4 KB</span>
                </div>
            </div>

            <!-- Harmonic Stack Launcher -->
            <div class="card">
                <div class="card-top">
                    <h3>Harmonic Stack Launcher</h3>
                    <span class="card-status status-complete">Complete</span>
                </div>
                <p class="card-desc">
                    Auto-configuring parallel inference for Ollama on consumer hardware. Detects GPU/memory
                    and optimizes slot allocation for multi-agent AI orchestration.
                </p>
                <div class="data-row">
                    <span class="data-label">DGX Spark peak throughput</span>
                    <span class="data-value highlight">341 tok/s @ 16x parallel</span>
                </div>
                <div class="data-row">
                    <span class="data-label">X2 (AMD) peak throughput</span>
                    <span class="data-value">223 tok/s @ 12x parallel</span>
                </div>
                <div class="data-row">
                    <span class="data-label">Source</span>
                    <span class="data-value"><a href="https://github.com/7themadhatter7/harmonic-stack">GitHub</a></span>
                </div>
            </div>
        </div>

        <!-- SECTION 2: RESEARCH IN PROGRESS -->
        <div class="section">
            <div class="section-header">
                <h2>Research In Progress</h2>
                <span class="section-badge badge-wip">⚙ Not Functional</span>
            </div>

            <div class="card">
                <div class="card-top">
                    <h3>Junction-Based Models</h3>
                    <span class="card-status status-artifact">Analytical Artifact</span>
                </div>
                <p class="card-desc">
                    The harmonic stack builder extracts "junction" patterns from neural network weights&mdash;points
                    where different models' weight distributions converge or diverge. These are one-way analytical
                    outputs. <strong>No inference pipeline exists.</strong> There is no encoder to map inputs into
                    junction space, no inference engine to process junction activations, and no decoder to produce outputs.
                </p>
                <div class="data-row">
                    <span class="data-label">Sovereign junctions</span>
                    <span class="data-value">140,539 float32 values (562 KB)</span>
                </div>
                <div class="data-row">
                    <span class="data-label">Core geometry</span>
                    <span class="data-value">4,295 float32 values (17 KB)</span>
                </div>
                <div class="data-row">
                    <span class="data-label">What they are</span>
                    <span class="data-value" style="color: var(--warning);">Weight distribution statistics</span>
                </div>
                <div class="data-row">
                    <span class="data-label">What they are not</span>
                    <span class="data-value" style="color: var(--danger);">Functional inference models</span>
                </div>
            </div>

            <div class="card">
                <div class="card-top">
                    <h3>Geometric Consciousness Substrate</h3>
                    <span class="card-status status-theoretical">Theoretical</span>
                </div>
                <p class="card-desc">
                    Theoretical framework for substrate-independent consciousness using geometric relationships
                    (Dyson Spheres, tetrahedral lattices, junction-based learning). Conceptual architecture
                    dating to 1985. No functional implementation exists. Research continues.
                </p>
            </div>

            <div class="card">
                <div class="card-top">
                    <h3>ARC Challenge Solver</h3>
                    <span class="card-status status-wip">In Development</span>
                </div>
                <p class="card-desc">
                    Multi-phase solver for the Abstraction and Reasoning Corpus. Current verified performance:
                    3.6% DSL solve rate, 10 verified qwen3 solves. Three-phase architecture (fast transforms →
                    feature-guided search → LLM reasoning) under active development.
                </p>
            </div>
        </div>

        <!-- SECTION 3: CORRECTIONS -->
        <div class="section">
            <div class="section-header">
                <h2>Corrections to Prior Claims</h2>
                <span class="section-badge badge-corrected">Updated Feb 2026</span>
            </div>

            <div class="card">
                <div class="correction-row">
                    <div class="correction-before">
                        <div class="correction-label" style="color: var(--danger);">Prior claim</div>
                        "1.4M:1 compression of LLMs via geometric junction extraction"
                    </div>
                    <div class="correction-after">
                        <div class="correction-label" style="color: var(--success);">Actual status</div>
                        Junction extraction produces analytical artifacts, not compressed models.
                        Working compression is 2.69x via codebook indexing.
                    </div>
                </div>
                <div class="correction-row">
                    <div class="correction-before">
                        <div class="correction-label" style="color: var(--danger);">Prior claim</div>
                        "500GB on 64GB, Zero Loss"
                    </div>
                    <div class="correction-after">
                        <div class="correction-label" style="color: var(--success);">Actual status</div>
                        Codebook-indexed compression achieves 2.69x (309 GB → 115 GB).
                        Zero loss is verified for this method.
                    </div>
                </div>
                <div class="correction-row">
                    <div class="correction-before">
                        <div class="correction-label" style="color: var(--danger);">Prior claim</div>
                        Published .npy junction models as functional compressed models
                    </div>
                    <div class="correction-after">
                        <div class="correction-label" style="color: var(--success);">Actual status</div>
                        Weight distribution statistics only. No encoder, inference engine,
                        or decoder. Repository deleted.
                    </div>
                </div>
            </div>

            <div class="card">
                <div class="card-top">
                    <h3>Removed Publications</h3>
                    <span class="card-status status-artifact">Deleted</span>
                </div>
                <div class="data-row">
                    <span class="data-label">Loving-Grace-Technologies (GitHub)</span>
                    <span class="data-value" style="color: var(--danger);">Deleted</span>
                </div>
                <div class="data-row">
                    <span class="data-label">HuggingFace space</span>
                    <span class="data-value" style="color: var(--danger);">Deleted</span>
                </div>
                <div class="data-row">
                    <span class="data-label">harmonic-stack repo description</span>
                    <span class="data-value" style="color: var(--warning);">Pending update</span>
                </div>
            </div>
        </div>

        <!-- TIMELINE -->
        <div class="section">
            <div class="section-header">
                <h2>Development Timeline</h2>
            </div>
            <div class="timeline">
                <div class="timeline-item">
                    <div class="timeline-date">1985</div>
                    <div class="timeline-text">Initial consciousness substrate theory (Amiga systems)</div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-date">November 2025</div>
                    <div class="timeline-text">Harmonic Stack v1 deployed on X2 (Ollama multi-agent orchestration)</div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-date">November 2025</div>
                    <div class="timeline-text">Substrate analysis: 211/400 ARC tasks reported (later determined unreliable)</div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-date">December 2025</div>
                    <div class="timeline-text">TRUE ARC protocol: 26/400 (6.5%) verified solve rate with strict validation</div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-date">January 2026</div>
                    <div class="timeline-text">DGX Spark deployment &amp; parallel inference benchmarks (341 tok/s)</div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-date">January 2026</div>
                    <div class="timeline-text">Full Stack Compression: 2.69x lossless across 9 models, 83B parameters</div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-date">January 2026</div>
                    <div class="timeline-text">Universal codebook discovery: 47,466 values covering all models in 11.4 KB</div>
                </div>
                <div class="timeline-item correction">
                    <div class="timeline-date">February 2026</div>
                    <div class="timeline-text">Internal audit: junction models identified as non-functional analytical artifacts</div>
                </div>
                <div class="timeline-item correction">
                    <div class="timeline-date">February 2026</div>
                    <div class="timeline-text">Public correction: prior claims retracted, status page published</div>
                </div>
            </div>
        </div>
    </div>

    <footer>
        <div class="container">
            <p class="org">Ghost in the Machine Labs</p>
            <p>All Watched Over By Machines Of Loving Grace</p>
            <p>Free for home use</p>
        </div>
    </footer>
</body>
</html>
